---
title: "Regression Trees and Rule-Based Models"
author: "Jose Zuniga"
output:
  html_document:
    toc: yes
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
if (!require("gbm")) install.packages("gbm")
if (!require("pls")) install.packages("pls")
if (!require("caret")) install.packages("caret")
if (!require("ipred")) install.packages("ipred")
if (!require("party")) install.packages("party")
if (!require("rpart")) install.packages("rpart")
if (!require("RWeka")) install.packages("RWeka")
if (!require("Cubist")) install.packages("caret")
if (!require("mlbench")) install.packages("mlbench")
if (!require("elasticnet")) install.packages("elasticnet")
if (!require("randomForest")) install.packages("randomForest")
if (!require("AppliedPredictiveModeling")) install.packages("AppliedPredictiveModeling")
library(gbm)
library(pls)
library(caret)
library(ipred)
library(party)
library(rpart)
library(RWeka)
library(Cubist)
library(mlbench)
library(elasticnet)
library(randomForest)
library(AppliedPredictiveModeling)
```

**Note**: Model descriptions are from *Applied Predictive Modeling* by Max Kuhn and Kjell Johnson. Solutions are modifications of those posted by Max Kuhn on his public [GitHub](https://github.com/topepo/APM_Exercises) page. Function descriptions are from the [RDocumentation](https://www.rdocumentation.org) website. 

## Classification and Regression Tree (CART)

**Single Decision Tree**. Tree-based models generate a set of conditions that are highly interpretable and are easy to implement. They can effectively handle many types of predictors (sparse, skewed, continuous, categorical, etc.) without the need to pre-process them (imputation, feature selection, etc.). In addition, these models do not require the user to specify the form of the predictors relationship to the response like, for example, a linear regression model requires. There are many techniques for constructing regression trees. One of the oldest and most utilized is the Classification and Regression Tree (**CART**) methodology also known as recursive partitioning. For regression, the model begins with the entire data set and searches every distinct value of every predictor to find the predictor and split value that partitions the data into two groups such that the overall sums of squares error are minimized. Then within each of [the] groups, this method searches for the predictor and split value that best reduces SSE until the number of samples in the splits falls below some threshold. The fully grown tree is then pruned back to a potentially smaller depth [to mitigate over-fitting]. If a predictor is never used in a split, the prediction equation is independent of those data.

## Conditional Inference Tree (CIT)

**Single Decision Tree**. Conditional Inference Trees (**CIT**) describe a unified framework for unbiased tree-based models for regression, classification, and other scenarios. Statistical hypothesis tests are used to do an exhaustive search across the predictors and their possible split points. For a candidate split, a statistical test is used to evaluate the difference between the means of the two groups created by the split and a $p$-value can be computed for the test. Multiple comparison corrections reduce the number of false-positive test results that are incurred by conducting [several] statistical hypothesis tests. This reduces bias for highly granular data. This algorithm does not use pruning. As data sets are split, the decrease in the number of samples reduces the power of the hypothesis tests. This results in higher $p$-values and a lower likelihood of a new split (and over-fitting). Statistical hypothesis tests are not directly related to predictive performance and it is therefore still advisable to choose the complexity of the tree on the basis of performance (via resampling or some other means).

## Regression Model Tree (M5)

**Single Decision Tree**. In Simple Regression Trees each terminal node uses the average of the training set outcomes in that node for prediction and therefore may not do a good job predicting samples whose true outcomes are extremely high or low.  The Regression Model Tree approach implemented with the **M5** algorithm uses different splitting criterion, predicts the outcome using a linear model, and results in a model that is often a combination of the predictions from different models along the same path through the tree. Like Simple Regression Trees, the initial split is found using an exhaustive search over the predictors and training set samples, but, unlike those models, the expected reduction in the node's error rate is used. Model Trees also incorporate a type of smoothing to decrease the potential for over-fitting. This smoothing can have a significant positive effect on the Model Tree when the linear models across nodes are very different.

## Bootstrap Aggregation (BAG)

**Ensemble Method**. Models based on single trees have particular weaknesses: (1) model instability (i.e., slight changes in the data can drastically change the structure of the tree or rules and, hence, the interpretation) and (2) less-than-optimal predictive performance. To combat these problems, researchers developed ensemble methods that combine many trees into one model. In the 1990s, ensemble techniques (methods that combine many models' predictions) began to appear. Bagging (**BAG**), short for <u>b</u>ootstrap <u>agg</u>regation, was one of the earliest developed ensemble techniques. Bagging is a general approach that uses bootstrapping in conjunction with any regression (or classification) model to construct an ensemble. Each model in the ensemble is used to generate a prediction for a new sample and these $m$ predictions are averaged to give the bagged model's prediction. For models that produce an unstable prediction, like regression trees, aggregating over many versions of the training data reduces the variance in the prediction and, hence, makes the prediction more stable. Bagging stable, lower variance models like [linear regression](https://rpubs.com/josezuniga/253955) and [MARS](https://rpubs.com/josezuniga/376348) offers less improvement in predictive performance. Although Bagging usually improves predictive performance for unstable models, results are less interpretable and computational costs are higher. CART (or conditional inference trees) can be used as the base learner in random forests.

## Random Forest (RF)

**Ensemble Method**. Generating bootstrap samples introduces a random component into the tree building process, which induces a distribution of trees, and therefore also a distribution of predicted values for each sample. The trees in Bagging, however, are not completely independent of each other since all the original predictors are considered at every split of every tree. This characteristic is known as tree correlation and prevents Bagging from optimally reducing variance of the predicted values. Random Forests (**RF**) are a unified model that [de-correlates trees by] adding randomness to the tree construction (learning) process [through implementation of] random split selection[, building] entire trees based on random subsets of descriptors[, and] adding noise to the response in order to perturb tree structure. This ensemble of many independent, strong learners yields an improvement in error rates that is robust to a noisy response. At the same time, the independence of learners can underfit data when the response is not noisy. Compared to Bagging, Random Forests is more computationally efficient on a tree-by-tree basis. The ensemble nature of Random Forests makes it impossible to gain an understanding of the relationship between the predictors and the response. CART (or conditional inference trees) can be used as the base learner in random forests.

## Stochastic Gradient Boosting (SGB)

**Ensemble Method**. Boosting models were originally developed for classification problems and were later extended to the regression setting. Boosting algorithms (gradient boosting machines) are influenced by learning theory [where] weak classifiers that predict marginally better than random are combined (or boosted) to produce an ensemble classifier with a superior generalized misclassification error rate. The basic principles of gradient boosting are as follows: given a loss function (e.g., squared error for regression) and a weak learner (e.g., regression trees), the algorithm seeks to find an additive model that minimizes the loss function. The algorithm is typically initialized with the best guess of the response (e.g., the mean of the response in regression). The gradient (e.g., residual) is calculated, and a model is then fit to the residuals to minimize the loss function. The current model is added to the previous model, and the procedure continues for a user-specified number of iterations. To remedy for greediness (choosing the optimal weak learner at each stage) is to constrain the learning process by employing regularization, or shrinkage [where] only a fraction of the current predicted value (commonly referred to as the learning rate) is added to the previous iteration's predicted value. Stochastic Gradient Boosting (**SGB**) implements this all using random sampling during selection the of training data. This [final] modification improves the prediction accuracy of Boosting while also reducing the required computational resources.

## Rule-Based Cubist (CUBE)

**Single Decision Tree or Ensemble Method**. A rule is defined as a distinct path through a tree. The number of samples affected by a rule is called its coverage. An initial [unsmoothed] model tree is created [and] only the rule with the largest coverage is saved from this model. The samples covered by the rule are removed from the training set and another model tree is created with the remaining data. Again, only the rule with the maximum coverage is retained. This process repeats until all the training set data have been covered by at least one rule. A new sample is predicted by determining which rule(s) it falls under then the linear model associated with the largest coverage is applied. Pruning has a large effect on the model and smoothing has a larger impact on unpruned models. Cubist (**CUBE**) is a rule-based model that is an amalgamation of several methodologies. [Previously] Cubist was only available in a commercial capacity, but in 2011 the [source code](http://www.rulequest.com) was released under an open-source license. Differences between Cubist and the previously described approaches for model trees and their rule-based variants are different techniques for linear model smoothing, creating rules, and pruning; an optional Boosting-like [ensemble] procedure called committees; predictions generated by the model rules can be adjusted using nearby points from the training set data. Cubist uses Manhattan (a.k.a. city block) distances to determine the nearest neighbors. Also, neighbors are only included if they are [not over the average distance] to the prediction sample.

## Exercise 8.1

Recreate the simulated data from [Exercise 7.2](http://rpubs.com/josezuniga/376348):

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
simulated <- mlbench.friedman1(200, sd = 1)
simulated <- cbind(simulated$x, simulated$y)
simulated <- as.data.frame(simulated)
colnames(simulated)[ncol(simulated)] <- "y"
```

### Exercise 8.1.a

Fit a random forest model to all of the predictors, then estimate the variable importance scores:

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
model1 <- randomForest(y ~ ., data=simulated, importance=TRUE, ntree=1000)
```
```{r warning=F, message=F, fig.align='center', cache=T}
varimp1 <- varImp(model1)
```

Did the random forest model significantly use the uninformative predictors (`V6` - `V10`)?

#### Approach

Methods for fitting the random forest model and the variable importance score calculations are given in the question setup. The function `randomForest()` is used for fitting the model. The parameter values `importance=TRUE` and `ntree=1000` dictate that the importance of predictors should be assessed and that the model should grow 1000 trees. The `varImp()` function calculates the variable importance for the model.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
varimp1
```

#### Interpretation

Per [Exercise 7.2](http://rpubs.com/josezuniga/376348), Friedman (1991) introduced several benchmark data sets created by simulation. One of these simulations used the following nonlinear equation to create data: $y=10\sin { \left( \pi x_{ 1 }x_{ 2 } \right) } +20\left( x_{ 3 }-0.5 \right) ^{ 2 }+10x_{ 4 }+5x_{ 5 }+N(0,\sigma ^{ 2 })$ where the $x$ values are random variables uniformly distributed between $[0,1]$ (there are also 5 other non-informative variables also created in the simulation). Applying the Random Forest model to these data and then calculating the variable importance reveals that the Random Forest model favors the informative predictors (`V1` - `V5`) and does not significantly use the uninformative predictors (`V6` - `V10`). 

### Exercise 8.1.b

Now add an additional predictor that is highly correlated with one of the informative predictors. For example:

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
duplicated <- simulated
duplicated$V11 <- duplicated$V1 + rnorm(200) * 0.1
cor(duplicated$V11, duplicated$V1)
```

Fit another random forest model to these data. Did the importance score for `V1` change? What happens when you add another predictor that is also highly correlated with `V1`?

#### Approach

Methods for adding an additional predictor that is highly correlated with one of the informative predictors are given in the question setup. This is done using the `rnorm()` function which produces pseudo-random Gaussian numbers. Specifically, the question setup adds a pseudo-random Gaussian number multiplied by 10% to each element in the `V1` vector to create the `V11` vector. The correlation between `V1` and `V11` is then confirmed with the base R `cor()` function. This new dataset from the question setup is fitted with a Random Forest model using the same `randomForest()` function. Again, the parameter values `importance=TRUE` and `ntree=1000` dictate that the importance of predictors should be assessed and that the model should grow 1000 trees. The `varImp()` function then calculates the variable importance for the new model. The variable importance calculations for both models are then output in a data frame for comparison. To create the data frame, the first model needs to be padded with a `V11` predictor that has a variable importance value of `NA`.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
model2 <- randomForest(y ~ ., data=duplicated, importance=TRUE, ntree=1000)
```
```{r warning=F, message=F, fig.align='center', cache=T}
varimp1 <- varImp(model1)
varimp2 <- varImp(model2)
colnames(varimp1) <- "Simulated" 
colnames(varimp2) <- "Correlated"
padding <- data.frame(Simulated=NA, row.names="V11")
data.frame(rbind(varimp1, padding), varimp2)
```

#### Interpretation

Adding the predictor `V11` that is highly correlated with the most informative predictor `V1` does change the variable importance score of `V1` as well as the importance scores of all the other informative and uninformative variables under the Random Forest (RF) model. The importance of this highly correlated `V11` predictor even outranks two of the five informative variables. This exercise highlights a weakness in tree-based models. When variables are highly correlated, tree-based models have a tendency to select more predictors than are actually needed with the choice between highly correlated predictors being nearly random.

### Exercise 8.1.c

Use the `cforest` function in the party package to fit a random forest model using conditional inference trees. The party package function `varimp` can calculate predictor importance. The `conditional` argument of that function toggles between the traditional importance measure and the modified version described in Strobl et al. (2007). Do these importances show the same pattern as the traditional random forest model?

#### Approach

The `cforest()` implementation of the random forest (and bagging) algorithm differs from `randomForest()` with respect to the base learners used and the aggregation scheme applied. Conditional inference Trees are fitted to each of the `ntree` (500 or defined via `cforest_control`) bootstrap samples of the learning sample. The hyperparameter in `cforest_control()` of `mtry` which dictates the number of randomly preselected variables is set to $P-1$, where $P$ is the number of predictors. After the `cforest()` model is fit, there are two functions that can calculate the importance of the variables. The `varimp()` function spelled all in lowercase from the `party` package calculates the standard and conditional variable importance for `cforest()` models following the permutation principle of the "mean decrease in accuracy" importance found in the `randomForest()` fucntion. The `varImp()` function with an uppercase **I** from the `caret` package is a generic method for calculating variable importance for objects produced by `train()` with other specific methods such as `cforest`. For the sake of consistency, the `varImp()` function with an uppercase **I** is used. Again, the variable importance calculations for all the models are then output in a data frame for comparison. To create the data frame, models without the `V11` predictor need to be padded with a `V11` predictor that has a variable importance value of `NA`.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
ctrl1 <- cforest_control(mtry = ncol(simulated) - 1)
fit1 <- cforest(y ~ ., data = simulated, controls = ctrl1)
ctrl2 <- cforest_control(mtry = ncol(duplicated) - 1)
fit2 <- cforest(y ~ ., data = duplicated, controls = ctrl2)
```
```{r warning=F, message=F, fig.align='center', cache=T}
varimp1 <- varImp(fit1)
varimp2 <- varImp(fit2)
varimp3 <- varImp(fit1, conditional=T)
varimp4 <- varImp(fit2, conditional=T)
colnames(varimp1) <- "Simulated_CIT_SI" 
colnames(varimp2) <- "Correlated_CIT_SI"
colnames(varimp3) <- "Simulated_CIT_CI" 
colnames(varimp4) <- "Correlated_CIT_CI"
padding1 <- data.frame(Simulated_CIT_SI=NA, row.names="V11")
padding2 <- data.frame(Simulated_CIT_CI=NA, row.names="V11")
data.frame(rbind(varimp1, padding1), rbind(varimp3, padding2), varimp2, varimp4)
```

#### Interpretation

Conditional variable importance scores are supposed to reflect “the true impact of each predictor variable more reliably than” the standard variable importance score calculation method. This however, does not seem to be the case here.  For the Conditional Inference Trees (CIT), the conditional importance (CI) scores follow the same ranking pattern, with respect to the informative variables for both the simulated and highly correlated data, as the standard importance (SI) scores. For the informative predictors the pattern is nearly with same as well. Inclusively, the drop in importance of the informative variables is greater when using the conditional importance (CI) scores.

### Exercise 8.1.d

Repeat this process with different tree models, such as boosted trees and Cubist. Does the same pattern occur?

#### Approach

The `bagging()` function from the `ipred` package conducts bagging for classification, regression and survival trees using a formula interface (`Y ~ X`) instead of a non-formula interface (`y=Y, x=X`) like the `ipredbagg()` function from the same package. The trees in the `bagging()` function are computed using the implementation in the `rpart` package.  The `nbagg` parameter specifies the number of bootstrap replications. The `gbm()` function fits generalized boosted regression models. The `distribution` parameter, which is dependent on the response variable, is set to Gaussian since the response is not of Bernoulli (2 classes), Multinomial (more than 2 classes), or Survival (right censored) types.  The `cubist()` function from the `Cubist` package  fits a rule-based M5 model with additional corrections based on nearest neighbors in the training set. The `committees` parameter specifies how many boosting interactions should be used in the ensemble. The Random Forest implementations `randomForest()` and `cforest()` are more flexible and reliable for computing Bootstrap Aggregated trees than the `bagging()` function and although they should generally be used instead of the `bagging()` function, the `bagging()` function is being used in this exercise for demonstration reasons. Again, the variable importance calculations for all the models are then output in a data frame for comparison. To create the data frame, models without the `V11` predictor need to be padded with a `V11` predictor that has a variable importance value of `NA`.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
fit3 <- bagging(y ~ ., simulated, nbagg = 50)
fit4 <- bagging(y ~ ., duplicated, nbagg = 50)
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
fit5 <- gbm(y ~ ., simulated, n.trees = 100, distribution = "gaussian")
fit6 <- gbm(y ~ ., duplicated, n.trees = 100, distribution = "gaussian")
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
fit7 <- cubist(x=simulated[,-11], y=simulated[,11], committees = 100)
fit8 <- cubist(x=duplicated[,-11], y=duplicated[,11], committees = 100)
```
```{r warning=F, message=F, fig.align='center', cache=T}
varimp1 <- varImp(fit3)
varimp2 <- varImp(fit4)
varimp3 <- varImp(fit5, numTrees = 100)
varimp4 <- varImp(fit6, numTrees = 100)
varimp5 <- varImp(fit7)
varimp6 <- varImp(fit8)
colnames(varimp1) <- "Sim_BAG"; colnames(varimp2) <- "Cor_BAG"
colnames(varimp3) <- "Sim_SGB"; colnames(varimp4) <- "Cor_SGB"
colnames(varimp5) <- "Sim_CUBE"; colnames(varimp6) <- "Cor_CUBE"
padding1 <- data.frame(Sim_BAG=NA, row.names="V11")
padding2 <- data.frame(Sim_SGB=NA, row.names="V11")
padding3 <- data.frame(Sim_CUBE=NA, row.names="V11")
data.frame(rbind(varimp1, padding1),  
  rbind(varimp3, padding2),
  rbind(varimp5, padding3), 
  varimp2, varimp4, varimp6)
```

#### Interpretation

Adding the predictor `V11` that is highly correlated with the most informative predictor `V1` has differing effects on the variable importance score of `V1` as well as the importance scores of all the other informative and uninformative variables under Bootstrap Aggregation (BAG), Stochastic Gradient Boosting (SGB), and Rule-Based Cubist (CUBE) models. The BAG method performs worse than the Random Forest (RF) model. Including the highly correlated variable makes the BAG model interpret the correlated variable `V11` as unimportant, but one of the uninformative variables (`V6`) as the most important. The SGB method performs poorly as well with one uninformative variables (`V10`) being listed as important and two informative variables (`V2`, `V5`) being listed as unimportant. The CUBE model fairs better than the other models by not marking any informative variables as unimportant, but it also lists an uninformative variables (`V10`) as the most important variable. This exercise highlights a weakness in tree-based models. When variables are highly correlated, tree-based models have a tendency to select more predictors than are actually needed with the choice between highly correlated predictors being nearly random.

## Exercise 8.2

Use a simulation to show tree bias with different granularities.

#### Approach

Single Regression Trees suffer from selection bias. Predictors with a higher number of distinct values (lower variance) are favored over more granular (higher variance) predictors. The danger occurs when a data set consists of a mix of informative and noise variables, and the noise variables have many more splits than the informative variables. There is therefore a high probability that the noise variables will be chosen to split the top nodes of the tree. Pruning will produce either a tree with misleading structure or no tree at all. Also, as the number of missing values increases, the selection of predictors becomes more biased. This phenomenon is best exemplified by creating a high variance predictor that is correlated to the response variable and a second predictor with less variance that is not related to the response variable. Specifically, a predictor `X1` with 100 one values and 100 two values is created. Then a response variable `Y` is created that is equal to `X1` plus a pseudo-random Gaussian variable with $\mu=0, \sigma=2$. A completely unrelated higher variance pseudo-random Gaussian variable `X2` with $\mu=0, \sigma=4$ is then created. These variables are then placed in a data frame and modeled using the `rpart()` function which makes splits based on the CART methodology. The `varImp()` function then calculates the variable importance for the model.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
X1 <- rep(1:2, each=100)
Y <- X1 + rnorm(200, mean=0, sd=4)
set.seed(624)
X2 <- rnorm(200, mean=0, sd=2)
simData <- data.frame(Y=Y, X1=X1, X2=X2)
set.seed(624)
fit <- rpart(Y ~ ., data = simData)
```
```{r warning=F, message=F, fig.align='center', cache=T}
varImp(fit)
```

#### Interpretation

The predictor `X1` which is related to `Y` is found to be less important than the higher variance `X2` predictor which is completely unrelated to `Y`. The selection bias pitfall of Single Regression Trees is highlighted well in this example. The importance of the unrelated `X2` predictor is not just greater, it is substantially greater. The calculated importance value of the unelated `X2` predictor is over five times larger than that of the related `X1` predictor.

## Exercise 8.3

In Stochastic Gradient Boosting the Bagging fraction and learning rate will govern the construction of the trees as they are guided by the gradient. Although the optimal values of these parameters should be obtained through the tuning process, it is helpful to understand how the magnitudes of these parameters affect magnitudes of variable importance. [The below generates] variable importance plots for boosting using two extreme values for the Bagging fraction (0.1 and 0.9) and the learning rate (0.1 and 0.9) for the solubility data.

```{r warning=F, message=F, fig.align='center', cache=T}
data(solubility)
X <- solTrainXtrans; Y <- solTrainY
indx <- createFolds(Y, returnTrain = T)
ctrl <- trainControl(method = "cv", index = indx)
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tg1 <- expand.grid(interaction.depth=seq(1,7,by=2), 
  n.trees=seq(100,1000,by=100), shrinkage = c(0.01, 0.1), n.minobsinnode=10)
tune1 <- train(X, Y, method="gbm", tuneGrid=tg1, trControl=ctrl, verbose=F)
int_dpth <- tune1$bestTune$interaction.depth
ntrees <- tune1$bestTune$n.trees
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tg2 <- expand.grid(interaction.depth=int_dpth,
  n.trees=ntrees, shrinkage=0.1, n.minobsinnode=10)
tune2 <- train(X, Y, method = "gbm", tuneGrid=tg2,
  trControl=ctrl, bag.fraction=0.1, verbose=F)
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tg3 <- expand.grid(interaction.depth=int_dpth,
  n.trees=ntrees, shrinkage=0.9, n.minobsinnode=10)
tune3 <- train(X, Y, method = "gbm", tuneGrid=tg3, 
  trControl=ctrl, bag.fraction=0.9, verbose=F)
```

A comparison of variable importance magnitudes for differing values of the Bagging fraction and shrinkage parameters. The left-hand plot has both [tuning] parameters set to 0.1, and the right-hand plot has both [tuning parameters] set to 0.9:

```{r warning=F, message=F, fig.align='center', cache=T}
varimp1 <- varImp(tune2, scale=F); varimp2 <- varImp(tune3, scale=F)
plot1 <- plot(varimp1, top=25, scales=list(y=list(cex=.95)))
plot2 <- plot(varimp2, top=25, scales=list(y=list(cex=.95)))
print(plot1, split=c(1,1,2,1), more=T); print(plot2, split=c(2,1,2,1))
```

### Exercise 8.3.a

Why does the model on the right focus its importance on just the first few of predictors, whereas the model on the left spreads importance across more predictors?

#### Approach

The `train()` function sets up a grid of tuning parameters for a number of classification and regression routines, fits each model, and calculates a resampling based performance measure. The `tuneGrid` parameter of the `train()` function is fed a data frame with values in columns named after the parameters being tuned. The hyperparameters declared in the tuning grid to be evaluated for the `gbm` (SGD) model are given in the question setup. Models with various variable interaction depths (`interaction.depth`), fitted trees (`n.trees`), learning rates (`shrinkage`), and a minimum of 10 observations in the trees terminal nodes (`n.minobsinnode`) are compared. Then the best interaction depth and number of iterations (fitted trees) from the tuning are examined more closely with their own SGB model. The first SGB model uses a 0.1 learning rate and selects 0.1 of the training set observations (`bag.fraction`) to propose the next tree in the expansion. The second SGB model uses a 0.9 learning rate and selects 0.9 of the training set observations (`bag.fraction`) to propose the next tree in the expansion.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
tune1$bestTune
varimp1 <- varImp(tune1)$importance
varimp2 <- varImp(tune2)$importance
df <- data.frame(sort(varimp1$Overall, T), sort(varimp2$Overall, T))
colnames(df) <- c("ShrinkBag_0.1" , "ShrinkBag_0.9")
head(df, 15)
```

#### Interpretation

This question is somewhat hard to answer. According to the author, model greediness (choosing the optimal weak learner at each stage) which decreases the number of predictors, increases proportional to the learning rate. Therefore, the lower learning rate of 0.1 in the left model results in fewer predictors than the model on the right which has a 0.9 learning rate. This number of predictors is also proportional to the stochastic nature of the model such that increased randomness sets the stage for an increased number of predictors. Therefore, the lower bagging fraction of 0.1 in the left model results in fewer predictors than the more deterministic model on the right which selects 0.9 of the training set observations to propose the next tree in the expansion.

### Exercise 8.3.b

Which model do you think would be more predictive of other samples?

#### Approach

The first SGB models uses a 0.1 learning rate and selects 0.1 of the training set observations (`bag.fraction`) to propose the next tree in the expansion. The second SGB models uses a 0.9 learning rate and selects 0.9 of the training set observations (`bag.fraction`) to propose the next tree in the expansion. The bagging fraction introduces randomness to the model that improves the predictive ability of the model.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
data.frame(model="SGB_0.1", tune2$bestTune, RMSE=min(tune2$results$RMSE), row.names="")
data.frame(model="SGB_0.9", tune3$bestTune, RMSE=min(tune3$results$RMSE), row.names="")
```

#### Interpretation

Greedy models are less likely to optimal global model and are prone to over-fitting. Stochastic models reduce prediction variance.  Therefore, the less greedy and more random model on the left with a 0.1 learning rate that selects 0.1 of the training set observations to propose the next tree in the expansion would be more predictive of other samples. The RMSE metric for the respective models supports this conclusion.

### Exercise 8.3.c

How would increasing interaction depth affect the slope of predictor importance for either model?

#### Approach

The first SGB models uses a 0.1 learning rate and selects 0.1 of the training set observations (`bag.fraction`) to propose the next tree in the expansion. The second SGB models uses a 0.9 learning rate and selects 0.9 of the training set observations (`bag.fraction`) to propose the next tree in the expansion.  The learning rate shrinks the RMSE by constraining the tree depth with a penalty.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
plot(tune1)
varimp1 <- varImp(tune1)
varimp2 <- varImp(tune2)
colnames(varimp1) <- "SGB_0.1"
colnames(varimp2) <- "SGB_0.9"
data.frame(arimp1, varimp2)
```

#### Interpretation

The positive impact of interaction depth on RMSE is proportional to the learning rate and number of trees. Models with a lower learning rate see greater improvements from interaction depth than models with a higher learning rate. A higher learning rate in these data results in improvements up to the 300 tree mark. Tree depth and learning rates impact the number of predictors which impacts the model greediness which impacts the predictive ability of the model which impacts the RMSE; all proportionally. Therefore, by transitivity, an increase in interaction depth will increase the number of predictors and RMSE. Variable importance will be spread across more predictors and the RMSE-Iterations graph will have a higher intercept that increases the slope.

## Exercise 8.4

Use a single predictor in the solubility data, such as the molecular weight or the number of carbon atoms and fit several models:

```{r warning=F, message=F, fig.align='center', cache=T}
data(solubility)
SOL_train_X <- subset(solTrainXtrans, select="MolWeight")
SOL_train_Y <- solTrainY
SOL_test_X <- subset(solTestXtrans, select="MolWeight")
SOL_test_Y <- solTestY
```

### Exercise 8.4.a

A simple regression tree.

#### Approach

The training and test sets are predefined by the `solubility` data. Selecting the molecular weight subset involves a simple application of the base R `subset()` function on the dataset where the `MolWeight` field is selected. The `train()` function sets up a grid of tuning parameters for a number of classification and regression routines, fits each model, and calculates a resampling based performance measure. CART can be implemented in the `train()` function using the `rpart` and the `rpart2` methods. The `rpart` method tunes the model over the complexity parameter (right-sized tree based on RMSE). The `rpart2` method tunes the model over the maximum depth (of the tree).

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tune1 <- train(SOL_train_X, SOL_train_Y, method = "rpart2", tuneLength = 1)
min(tune1$results$RMSE)
```

#### Interpretation

The CART model applied to the Molecular Weight predictor in the `solubility` dataset results in a 1.663586 RMSE.

### Exercise 8.4.b

A random forest model.

#### Approach

The training and test sets are predefined by the `solubility` data. Selecting the molecular weight subset involves a simple application of the base R `subset()` function on the dataset selecting the `MolWeight` field. The `train()` function sets up a grid of tuning parameters for a number of classification and regression routines, fits each model, and calculates a resampling based performance measure. The `rf` method in the `train()` function implements Random Forest classification and regression using the `randomForest` package.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tune2 <- train(SOL_train_X, SOL_train_Y, method = "rf", tuneLength = 1)
min(tune2$results$RMSE)
```

#### Interpretation

The RF model applied to the Molecular Weight predictor in the `solubility` dataset results in a 1.562868 RMSE.

### Exercise 8.4.c

Different Cubist models with a single rule or multiple committees (each with and without using neighbor adjustments).

#### Approach

The training and test sets are predefined by the `solubility` data. Selecting the molecular weight subset involves a simple application of the base R `subset()` function on the dataset selecting the `MolWeight` field. The `train()` function sets up a grid of tuning parameters for a number of classification and regression routines, fits each model, and calculates a resampling based performance measure. The `tuneGrid` parameter of the `train()` function is fed a data frame with values in columns named after the parameters being tuned. The `cubist` method in the `train()` function implements a rule-based M5 model with additional corrections based on nearest neighbors in the training set using the `Cubist` package.  The hyperparameters declared in the tuning grid to be evaluated are the `committees` parameter which specifies how many boosting interactions should be used in the ensemble, and the `neighbors` parameter which specifies the number of instances to use to correct the rule-based prediction.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tg3 <- expand.grid(committees = 1, neighbors = 0)
tune3 <- train(SOL_train_X, SOL_train_Y, method="cubist", verbose=F, metric="Rsquared", tuneGrid=tg3)
min(tune3$results$RMSE)
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tg4 <- expand.grid(committees = 1, neighbors = c(1,3,5,7))
tune4 <- train(SOL_train_X, SOL_train_Y, method="cubist", verbose=F, metric="Rsquared", tuneGrid=tg4)
min(tune4$results$RMSE)
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tg5 <- expand.grid(committees = 100, neighbors = 0)
tune5 <- train(SOL_train_X, SOL_train_Y, method="cubist", verbose=F, metric="Rsquared", tuneGrid=tg5)
min(tune5$results$RMSE)
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tg6 <- expand.grid(committees = 100, neighbors = c(1,3,5,7))
tune6 <- train(SOL_train_X, SOL_train_Y, method="cubist", verbose=F, metric="Rsquared", tuneGrid=tg6)
min(tune6$results$RMSE)
```

#### Interpretation

The CUBE models applied to the Molecular Weight predictor in the `solubility` dataset result in a RMSE ranging between [1.528700, 1.602018]

### Exercise 8.4.d

Plot the predictor data versus the solubility results for the test set. Overlay the model predictions for the test set. How do the models differ? Does changing the tuning parameter(s) significantly affect the model fit?

#### Approach

The `predict()` function applies a fitted model to data consisting of predictor variables. A function is created to plot the predictor test data versus the solubility test results then overlay the predictions of the six models. The `postResample()` function takes two vectors as its inputs and computes the RMSE, $R^2$, and MAE performance metrics.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
models <- c("CART","RF","CUBE_1.0","CUBE_1.n","CUBE_C.0","CUBE_C.n")
fcast1 <- predict(tune1, newdata = SOL_test_X)
fcast2 <- predict(tune2, newdata = SOL_test_X)
fcast3 <- predict(tune3, newdata = SOL_test_X)
fcast4 <- predict(tune4, newdata = SOL_test_X)
fcast5 <- predict(tune5, newdata = SOL_test_X)
fcast6 <- predict(tune6, newdata = SOL_test_X)
fcast <- cbind(fcast1, fcast2, fcast3, fcast4, fcast5, fcast6)
par(mfrow=c(3, 2), mar = c(0, 1, 2, 0), oma = c(0, 0, 0.5, 0.5))
for (i in 1:6) {
  X <- SOL_test_X$MolWeight; Y <- SOL_test_Y
  plot(X, Y, xaxt="n", yaxt="n", xlab="", ylab="", main=models[i])
  par(new=T); 
  plot(X, fcast[,i], xaxt="n", yaxt="n", xlab="", ylab="", col="blue")
  
}
data.frame(row.names = models, rbind(
  postResample(pred = fcast1, obs = SOL_test_Y), 
  postResample(pred = fcast2, obs = SOL_test_Y), 
  postResample(pred = fcast3, obs = SOL_test_Y),
  postResample(pred = fcast4, obs = SOL_test_Y),
  postResample(pred = fcast5, obs = SOL_test_Y),
  postResample(pred = fcast6, obs = SOL_test_Y)))
```

#### Interpretation

Overlaying the model predictions on the test set is very useful in highlighting the differences in the models. The CART model predicts values within two categories that does not fit the data very well. The RF model fits the data much better but its predictions are the most heteroskedastic of all the fitted models. The CUBE models appear perform the best but there are differences between them. Although the number of committees which specifies how many boosting interactions should be used in the ensemble has little to no effect in the CUBE model predictions, the number of neighbors which specifies the number of instances to use to correct the rule-based prediction has a significant impact. Declaring zero neighbors makes the CUBE model predictions homoskedastic to a fault. Multiple neighbors adds variance to the prediction which improves the fit. Interestingly enough, the RMSE, $R^2$ and MAE performance metrics indicate that the RF model with the most heteroskedasticity is the best fit for these data.

## Exercise 8.5

Fit different tree- and rule-based models for the Tecator data discussed [below]. How do they compare to linear models? Do the between-predictor correlations seem to affect your models? If so, how would you transform or re-encode the predictor data to mitigate this issue?

> Infrared (IR) spectroscopy technology is used to determine the chemical makeup of a substance. The theory of IR spectroscopy holds that unique molecular structures absorb IR frequencies differently. In practice a spectrometer fires a series of IR frequencies into a sample material, and the device measures the absorbance of the sample at each individual frequency. This series of measurements creates a spectrum profile which can then be used to determine the chemical makeup of the sample material. A Tecator Infratec Food and Feed Analyzer instrument was used to analyze 215 samples of meat across 100 frequencies. In addition to an IR profile, analytical chemistry determined the percent content of water, fat, and protein for each sample. If we can establish a predictive relationship between IR spectrum and fat content, then food scientists could predict a sample's fat content with IR instead of using analytical chemistry. This would provide costs savings, since analytical chemistry is a more expensive, time-consuming process.

```{r warning=F, message=F, fig.align='center', cache=T}
data(tecator)
set.seed(624)
rows_train <- createDataPartition(endpoints[, 3], p = 3/4, list= FALSE)
TEC_train_X <- as.data.frame(absorp[rows_train, ])
TEC_test_X <- as.data.frame(absorp[-rows_train, ])
TEC_train_Y <- endpoints[rows_train, 3]
TEC_test_Y <- endpoints[-rows_train, 3]
ctrl <- trainControl(method = "repeatedcv", repeats = 5)
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tune1 <- train(x = TEC_train_X, y = TEC_train_Y, method="pls",
  trControl = ctrl, preProcess = c("center", "scale"), tuneLength = 25)
```

#### Approach

The `tecator` data partitioning, controls, and PLS model are given in the question setup. The `createDataPartition()` function helps create a series of test and training partitions. The `p` parameter sets the percentage of data that goes to training. Declaring `list=FALSE` returns a matrix.  The `trainControl()` function controls the computational nuances of  the `train()` function. The `repeatedcv` resampling method and `number` parameters are specific the type of $k$-fold cross-validation to be implemented. The `train()` function sets up a grid of tuning parameters for a number of classification and regression routines, fits each model, and calculates a resampling based performance measure. The `tuneGrid` parameter of the `train()` function is fed a data frame with values in columns named after the parameters being tuned.  CART is implemented in the `train()` function using the `rpart` method which tunes the model over the complexity parameter (right-sized tree based on RMSE). BAG is implemented in the `train()` function using the `treebag` method which conducts bagging for classification, regression and survival trees using the implementation in the `rpart` package. RF is implemented in the `train()` function using the `rf` method which implements Random Forest classification and regression using the `randomForest` package.  SGB is implemented in the `train()` function using the `gbm` method fits generalized boosted regression models. The hyperparameters declared in the tuning grid to be evaluated are various variable interaction depths (`interaction.depth`), fitted trees (`n.trees`), learning rates (`shrinkage`), and a minimum of 10 observations in the trees terminal nodes (`n.minobsinnode`). CUBE is implemented in the `train()` function using the `cubist` method which fits a rule-based M5 model with additional corrections based on nearest neighbors in the training set. The hyperparameters declared in the tuning grid to be evaluated are the `committees` parameter which specifies how many boosting interactions should be used in the ensemble, and the `neighbors` parameter which specifies the number of instances to use to correct the rule-based prediction. A function is created to help display the resampling performance metrics as a data frame. The `bwplot()` function plots a series of vertical box-and-whisker plots for the resampling performance metrics.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tune2 <- train(x = TEC_train_X, y = TEC_train_Y, 
  method="rpart", trControl = ctrl, tuneLength = 25)
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tune3<- train(x = TEC_train_X, y = TEC_train_Y, 
  method="treebag", trControl = ctrl)
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tune4<- train(x = TEC_train_X, y = TEC_train_Y, method = "rf",
  ntree = 1500, tuneLength = 10, trControl = ctrl)
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tg5 <- expand.grid(interaction.depth = seq(1, 7, by = 2),
  n.trees = seq(100, 1000, by = 50), shrinkage = c(0.01, 0.1), n.minobsinnode=10)
tune5 <- train(x = TEC_train_X, y = TEC_train_Y, method = "gbm",
  verbose = FALSE, tuneGrid = tg5, trControl = ctrl)
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tg6 <- expand.grid(committees = c(1:10, 20, 50, 75, 100), neighbors = c(0, 1, 5, 9))
tune6 <- train(x = TEC_train_X, y = TEC_train_Y, method = "cubist",
  verbose = FALSE, tuneGrid = tg6, trControl = ctrl)
```

##### Training Set Resampling

```{r warning=F, message=F, fig.align='center', cache=T, results = 'hold'}
metrics <- function(tune) {
  RMSE = min(tune$results$RMSE)
  Rsquared = max(tune$results$Rsquared)
  MAE = min(tune$results$MAE)
  return(cbind(RMSE, Rsquared, MAE)) }
data.frame(rbind(metrics(tune1), metrics(tune2), metrics(tune3), 
  metrics(tune4), metrics(tune5), metrics(tune6)), 
  row.names = c("PLS","CART","BAG","RF","SGB","CUBE"))
fits <- list(PLS=tune1, CART=tune2, BAG=tune3, RF=tune4, SGB=tune5, CUBE=tune6)
bwplot(resamples(fits))
```

#### Interpretation

After fitting a CART, BAG, RF, SGB, and CUBE models to the `tecator` data which were previously found to be best suited for fitting with a linear PLS model, it appears that a CUBE model may also provide a suitable fit for the  `tecator` data. The CUBE and PLS models outperform the other tree-based models in every resampling performance metric. The RMSE and $R^2$ resampling performance metrics of the PLS model however, outperforms the CUBE model. The CUBE model only outperforms the PLS model in the MAE resampling performance metric. The linear PLS model still appears to provide the best fit for the `tecator` data although the CUBE model comes in at a close second. In an earlier exercise it was found that predictor correlations do have an impact CUBE models so removing highly correlated variables using `findCorrelation(cor(.))` may yield improvements that can make a CUBE model a better fit for the `tecator` data that a PLS model.

## Exercise 8.6

Return to the permeability problem described [below]. Train several tree-based models and evaluate the resampling and test set performance:

> Permeability is the measure of a molecule's ability to cross a membrane. Compounds that appear to be effective for a particular disease in research screening experiments but appear to be poorly permeable may need to be altered in order to improve permeability and thus the compound's ability to reach the desired target. Permeability assays such as PAMPA and Caco-2 have been developed to help measure compounds' permeability. These screens are effective at quantifying a compound's permeability, but the assay is expensive labor intensive. Given a sufficient number of compounds that have been screened, we could develop a predictive model for permeability in an attempt to potentially reduce the need for the assay. In this project there were 165 unique compounds; 1,107 molecular fingerprints were determined for each. A molecular fingerprint is a binary sequence of numbers that represents the presence or absence of a specific molecular substructure. The response is highly skewed, the predictors are sparse, and many predictors are strongly associated. Developing a model to predict permeability could save significant resources for a pharmaceutical company, while at the same time more rapidly identifying molecules that have a sufficient permeability to become a drug.

```{r warning=F, message=F, fig.align='center', cache=T}
library(AppliedPredictiveModeling)
data(permeability)
nzv <- nearZeroVar(fingerprints)
fingerprints <- fingerprints[,-nzv]
set.seed(624)
rows_train <- createDataPartition(permeability, p = 0.75, list = FALSE)
PER_train_X <- fingerprints[rows_train,]
PER_train_Y <- permeability[rows_train,]
PER_test_X <- fingerprints[-rows_train,]
PER_test_Y <- permeability[-rows_train,]
ctrl <- trainControl(method = "LGOCV")
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tg1 <- expand.grid(lambda = c(0, 0.05, .1), fraction = seq(.05, 1, length = 25))
tune1 <- train(x = PER_train_X, y = log10(PER_train_Y), 
  method = "enet", tuneGrid = tg1, trControl = ctrl)
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tg2 <- expand.grid(sigma = c(0.0005,0.001,0.0015), C = seq(1,49,by=6))
tune2 <- train(x = PER_train_X, y = log10(PER_train_Y), 
  method = "svmRadial", tuneGrid = tg2, trControl = ctrl)
```

### Exercise 8.6.a

Which tree-based model gives the optimal resampling and test set performance?

#### Approach

The `permeability` data partitioning, transformation, controls, ENET model, and SVM model are given in the question setup. The `createDataPartition()` function helps create a series of test and training partitions. The `p` parameter sets the percentage of data that goes to training. Declaring `list=FALSE` returns a matrix.  The `trainControl()` function controls the computational nuances of  the `train()` function. The `LGOCV` resampling method and `number` parameters specific the type of leave-group-out cross-validation to be implemented. The `train()` function sets up a grid of tuning parameters for a number of classification and regression routines, fits each model, and calculates a resampling based performance measure. The `tuneGrid` parameter of the `train()` function is fed a data frame with values in columns named after the parameters being tuned.  CART is implemented in the `train()` function using the `rpart2` method which tunes the model over the maximum depth (of the tree). RF is implemented in the `train()` function using the `rf` method which implements Random Forest classification and regression using the `randomForest` package.  SGB is implemented in the `train()` function using the `gbm` method fits generalized boosted regression models. The hyperparameters declared in the tuning grid to be evaluated are various variable interaction depths (`interaction.depth`), fitted trees (`n.trees`), learning rates (`shrinkage`), and a minimum of 10 observations in the trees terminal nodes (`n.minobsinnode`). A function is created to help display the resampling performance metrics as a data frame. The `bwplot()` function plots a series of vertical box-and-whisker plots for the resampling performance metrics. The `predict()` function applies a fitted model to data consisting of predictor variables. A data frame with the test set performance metrics is then displayed.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tg3 <- expand.grid(maxdepth= seq(1,10,by=1))
tune3 <- train(x = PER_train_X, y = log10(PER_train_Y),
  method = "rpart2", tuneGrid = tg3, trControl = ctrl)
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tune4 <- train(x = PER_train_X, y = log10(PER_train_Y),
  method = "rf", tuneLength = 10, importance = TRUE, trControl = ctrl)
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tg5 <- expand.grid(interaction.depth=seq(1,6,by=1), n.minobsinnode=10, 
  n.trees=c(25,50,100,200), shrinkage=c(0.01,0.05,0.1))
tune5 <- train(x = PER_train_X, y = log10(PER_train_Y),
  method = "gbm", verbose = F, tuneGrid = tg5, trControl = ctrl)
```

##### Training Set Resampling

```{r warning=F, message=F, fig.align='center', cache=T, results = 'hold'}
metrics <- function(tune) {
  RMSE = min(tune$results$RMSE)
  Rsquared = max(tune$results$Rsquared)
  MAE = min(tune$results$MAE)
  return(cbind(RMSE, Rsquared, MAE)) }
data.frame(rbind(metrics(tune1), metrics(tune2), 
  metrics(tune3), metrics(tune4), metrics(tune5)),
  row.names = c("ENET","SVM","CART","RF","SGB"))
fits <- list(ENET=tune1, SVM=tune2, CART=tune3, RF=tune4, SGB=tune5)
bwplot(resamples(fits))
```

##### Validation on Test Set 

```{r warning=F, message=F, fig.align='center', cache=T}
fcast1 <- predict(tune1, newdata = PER_test_X)
fcast2 <- predict(tune2, newdata = PER_test_X)
fcast3 <- predict(tune3, newdata = PER_test_X)
fcast4 <- predict(tune4, newdata = PER_test_X)
fcast5 <- predict(tune5, newdata = PER_test_X)
data.frame(rbind(postResample(pred = fcast1, obs = PER_test_Y), 
  postResample(pred = fcast2, obs = PER_test_Y), 
  postResample(pred = fcast3, obs = PER_test_Y),
  postResample(pred = fcast4, obs = PER_test_Y),
  postResample(pred = fcast5, obs = PER_test_Y)), 
  row.names = c("ENET","SVM","CART","RF","SGB"))
```

#### Interpretation

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

### Exercise 8.6.b

Do any of these models outperform the covariance or non-covariance based regression models you have previously developed for these data? What criteria did you use to compare models' performance?

#### Approach

A function is created to help display the resampling performance metrics as a data frame. The `bwplot()` function plots a series of vertical box-and-whisker plots for the resampling performance metrics. 

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
metrics <- function(tune) {
  RMSE = min(tune$results$RMSE)
  Rsquared = max(tune$results$Rsquared)
  MAE = min(tune$results$MAE)
  return(cbind(RMSE, Rsquared, MAE)) }
data.frame(rbind(metrics(tune1), metrics(tune2), 
  metrics(tune3), metrics(tune4), metrics(tune5)),
  row.names = c("ENET","SVM","CART","RF","SGB"))
fits <- list(ENET=tune1, SVM=tune2, CART=tune3, RF=tune4, SGB=tune5)
bwplot(resamples(fits))
```

#### Interpretation

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

### Exercise 8.6.c

Of all the models you have developed thus far, which, if any, would you recommend to replace the permeability laboratory experiment?

#### Approach

The `train()` function, once computed and stored to a variable, contains several attributes which help evaluate model performance.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T, results = 'hold'}
tune3$modelInfo$label
data.frame(model="CART", tune3$bestTune, RMSE=min(tune3$results$RMSE), row.names="")
plot(tune3)
```

#### Interpretation

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

## Exercise 8.7

Refer to Exercise [6.3](http://rpubs.com/josezuniga/366918) and [7.5](https://rpubs.com/josezuniga/376348) which describe a chemical manufacturing process. Use the same data imputation, data splitting, and pre-processing steps as before and train several tree-based models:

```{r warning=F, message=F, fig.align='center', cache=T}
CMP <- get(data(ChemicalManufacturingProcess))
rm(ChemicalManufacturingProcess) # list not needed
set.seed(624)
rows_train <- createDataPartition(CMP$Yield, p=0.75, list=F)
CMP_train <- CMP[rows_train, ]
CMP_test <- CMP[-rows_train, ]
prepro <- preProcess(subset(CMP_train, select = - Yield),
  method=c("BoxCox", "center", "scale", "knnImpute"))
CMP_train_X <- predict(prepro, CMP_train[ , -1])
nzv <- nearZeroVar(CMP_train_X)
mcl <- findCorrelation(cor(CMP_train_X))
CMP_train_X <- CMP_train_X[ ,-c(nzv, mcl)] 
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
ctrl1 <- trainControl(method = "boot", number = 25)
tune1 <- train(x = CMP_train_X, y = CMP_train$Yield,
  method="pls", tuneLength=15, trControl=ctrl1)
```
```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
ctrl2 <- trainControl(method = "cv")
tg2 <- data.frame(.k = 1:20)
tune2 <- train(x = CMP_train_X, y = CMP_train$Yield,
  method = "knn", tuneGrid = tg2, trControl = ctrl2)
```

#### Classification and Regression Tree (CART)

CART is implemented in the `train()` function using the `rpart2` method which tunes the model over the maximum depth (of the tree). 

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
ctrl3 <- trainControl(method = "boot", number = 25)
tg3 <- expand.grid(maxdepth= seq(1,10,by=1))
tune3 <- train(x = CMP_train_X, y = CMP_train$Yield, method = "rpart2",
  metric = "Rsquared", tuneGrid = tg3, trControl = ctrl3)
```

#### Conditional Inference Tree (CIT)

CIT is implemented in the `train()` function using the `ctree2` method where the number of randomly selected predictors to choose from at each split is set by `mtry`. Since Random Forests is computationally intensive, `mtry` is tuned with around five values that are somewhat evenly spaced across the range from 2 to $P$, where $P$ is the number of predictors.

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
ctrl4 <- trainControl(method = "cv")
tg4 <- expand.grid(maxdepth= seq(1,10,by=1), mincriterion=1-c(0.01, 0.05, 0.1))
tune4 <- train(x = CMP_train_X, y = CMP_train$Yield, method = "ctree2",
  metric = "Rsquared", tuneGrid = tg4, trControl = ctrl4)
```

#### Regression Model Tree (M5)

M5 is implemented in the `train()` function using the `M5` method from the `RWeka` package. The `prune`, `smooth`, and `rules` hyperparameters evaluate models with and without those features.

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
yn <- c("Yes","No")
ctrl5 <- trainControl(method = "cv")
tg5 <- expand.grid(pruned = yn, smoothed = yn, rules = yn)
tune5 <- train(x = CMP_train_X, y = CMP_train$Yield, method = "M5",
  metric = "Rsquared", tuneGrid = tg5, trControl = ctrl5)
```

#### Bootstrap Aggregation (BAG)

BAG is implemented in the `train()` function using the `bag` method which conducts bagging for classification and regression using the implementation in the `caret` package. The `bagControl()` function controls the computational nuances of this `train()` function method. The hyperparameters declared in the tuning grid are fit, predict and aggregate which all take a function that provides a framework for bagging classification or regression models. The function used for the bagging predictions is the tree-based function `ctreeBag`.

```{r warning=F, message=F, fig.align='center', cache=T}
n <- ncol(CMP_train_X) 
seq_len(abs(n))[n %% seq_len(abs(n)) == 0L]
set.seed(624)
bag6 <- bagControl(fit=ctreeBag$fit, predict=ctreeBag$pred, aggregate=ctreeBag$aggregate)
ctrl6 <-trainControl(method = "cv")
tg6 <- data.frame(vars = seq(2, n, 11))
tune6 <- train(x = CMP_train_X, y = CMP_train$Yield, method = "bag", B = 25, 
  metric = "Rsquared", tuneGrid = tg6, trControl = ctrl6, bagControl = bag6)
```

#### Random Forest (RF)

RF is implemented in the `train()` function using the `rf` method which implements Random Forest classification and regression using the `randomForest` package. 

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
ctrl7 <- trainControl(method = "boot", number = 25)
tg7 <- expand.grid(mtry=seq(2,38,by=3))
tune7 <- train(x = CMP_train_X, y = CMP_train$Yield, method = "rf", 
  metric = "Rsquared", tuneGrid = tg7, trControl = ctrl7)
```

#### Stochastic Gradient Boosting (SGB)

SGB is implemented in the `train()` function using the `gbm` method fits generalized boosted regression models. The hyperparameters declared in the tuning grid to be evaluated are various variable interaction depths (`interaction.depth`), fitted trees (`n.trees`), learning rates (`shrinkage`), and a minimum of 10 observations in the trees terminal nodes (`n.minobsinnode`). 

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
ctrl8 <- trainControl(method = "boot", number = 25)
tg8 <- expand.grid(interaction.depth=seq(1,6,by=1), n.trees=c(25,50,100,200),
  shrinkage=c(0.01,0.05,0.1,0.2), n.minobsinnode=10)
tune8 <- train(x = CMP_train_X, y = CMP_train$Yield, method = "gbm", 
  metric = "Rsquared", tuneGrid = tg8, trControl = ctrl8, verbose=F)
```

#### Rule-Based Cubist (CUBE)

CUBE is implemented in the `train()` function using the `cubist` method which fits a rule-based M5 model with additional corrections based on nearest neighbors in the training set. The hyperparameters declared in the tuning grid to be evaluated are the `committees` parameter which specifies how many boosting interactions should be used in the ensemble, and the `neighbors` parameter which specifies the number of instances to use to correct the rule-based prediction. 

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
ctrl9 <- trainControl(method = "boot", number = 25)
tg9 <- expand.grid(committees = c(1,5,10,20,50,100), neighbors = c(0,1,3,5,7))
tune9 <- train(x = CMP_train_X, y = CMP_train$Yield, method = "cubist", 
  metric = "Rsquared", tuneGrid = tg9, trControl = ctrl9)
```

### Exercise 8.7.a

Which tree-based regression model gives the optimal resampling and test set performance?

#### Approach

The `ChemicalManufacturingProcess` data partitioning, preprocessing, controls, PLS model, and KNN model are given in the question setup. The `train()` function, once computed and stored to a variable, contains several attributes which help evaluate model performance. A function is created to help display the resampling performance metrics as a data frame. A slight issue arises during forecasting due to a value of $-\infty$ remaining in the pre-processed dataset. This infinite value hinders forecasting with the Neural Network and $K$-Nearest Neighbor models but does not affect the Multivariate Adaptive Regression Splines and Support Vector Machine models. For the sake of completeness in model comparison, the value of $-\infty$ is replaced with the minimum finite value of the variable to which the infinite value belongs. The `predict()` function applies a fitted model to data consisting of predictor variables. The `postResample()` function takes two vectors as its inputs and computes the RMSE, $R^2$, and MAE performance metrics. A data frame with the test set performance metrics is then displayed.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T, results = 'hold'}
tune3$modelInfo$label
data.frame(model="CART", tune3$bestTune, RMSE=min(tune3$results$RMSE), row.names="")
plot(tune3)
```
```{r warning=F, message=F, fig.align='center', cache=T, results = 'hold'}
tune4$modelInfo$label
data.frame(model="CIT", tune4$bestTune, RMSE=min(tune4$results$RMSE), row.names="")
plot(tune4)
```
```{r warning=F, message=F, fig.align='center', cache=T, results = 'hold'}
tune5$modelInfo$label
data.frame(model="M5", tune5$bestTune, RMSE=min(tune5$results$RMSE), row.names="")
plot(tune5)
```
```{r warning=F, message=F, fig.align='center', cache=T, results = 'hold'}
tune6$modelInfo$label
data.frame(model="BAG", tune5$bestTune, RMSE=min(tune6$results$RMSE), row.names="")
plot(tune6)
```
```{r warning=F, message=F, fig.align='center', cache=T, results = 'hold'}
tune7$modelInfo$label
data.frame(model="RF", tune7$bestTune, RMSE=min(tune7$results$RMSE), row.names="")
plot(tune7)
```
```{r warning=F, message=F, fig.align='center', cache=T, results = 'hold'}
tune8$modelInfo$label
data.frame(model="SGB", tune8$bestTune, RMSE=min(tune8$results$RMSE), row.names="")
plot(tune8)
```
```{r warning=F, message=F, fig.align='center', cache=T, results = 'hold'}
tune9$modelInfo$label
data.frame(model="CUBE", tune9$bestTune, RMSE=min(tune9$results$RMSE), row.names="")
plot(tune9)
```

##### Training Set Resampling

```{r warning=F, message=F, fig.align='center', cache=T, results = 'hold'}
metrics <- function(tune) {
  RMSE = min(tune$results$RMSE)
  Rsquared = max(tune$results$Rsquared)
  MAE = min(tune$results$MAE)
  return(cbind(RMSE, Rsquared, MAE)) }
data.frame(rbind(metrics(tune1), metrics(tune2),
  metrics(tune3), metrics(tune4), metrics(tune5),
  metrics(tune6), metrics(tune7), metrics(tune8), metrics(tune9)),
  row.names = c("PLS","KNN","CART","CIT","M5","BAG","RF","SGB","CUBE"))
```

##### Validation on Test Set 

```{r warning=F, message=F, fig.align='center', cache=T}
CMP_test_X <- predict(prepro, CMP_test[ , -1])
CMP_test_X <- CMP_test_X[ ,-c(nzv, mcl)] 
val <- min(CMP_test_X[is.finite(CMP_test_X[,35]), 35])
CMP_test_X[CMP_test_X[,35] == -Inf, 35] <- val
fcast1 <- predict(tune1, newdata = CMP_test_X)
fcast2 <- predict(tune2, newdata = CMP_test_X)
fcast3 <- predict(tune3, newdata = CMP_test_X)
fcast4 <- predict(tune4, newdata = CMP_test_X)
fcast5 <- predict(tune5, newdata = CMP_test_X)
fcast6 <- predict(tune6, newdata = CMP_test_X)
fcast7 <- predict(tune7, newdata = CMP_test_X)
fcast8 <- predict(tune8, newdata = CMP_test_X)
fcast9 <- predict(tune9, newdata = CMP_test_X)
data.frame(rbind(postResample(pred = fcast1, obs = CMP_test$Yield), 
  postResample(pred = fcast2, obs = CMP_test$Yield), 
  postResample(pred = fcast3, obs = CMP_test$Yield),
  postResample(pred = fcast4, obs = CMP_test$Yield),
  postResample(pred = fcast5, obs = CMP_test$Yield),
  postResample(pred = fcast6, obs = CMP_test$Yield),
  postResample(pred = fcast7, obs = CMP_test$Yield),
  postResample(pred = fcast8, obs = CMP_test$Yield),
  postResample(pred = fcast9, obs = CMP_test$Yield)), 
  row.names =  c("PLS","KNN","CART","CIT","M5","BAG","RF","SGB","CUBE"))
```

#### Interpretation

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

### Exercise 8.7.b

Which predictors are most important in the optimal tree-based regression model? Do either the biological or process variables dominate the list? How do the top 10 important predictors compare to the top 10 predictors from the optimal linear and nonlinear models?

#### Approach

The `dotPlot()` function create a dotplot of variable importance values. The `varImp()` function calculates the variable importance for a given model.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
dotPlot(varImp(tune6), top=15)
varimp1 <- varImp(tune9)$importance
varimp2 <- varImp(tune1)$importance
varimp3 <- varImp(tune2)$importance
colnames(varimp1) <- "CUBE"
colnames(varimp2) <- "PLS"
colnames(varimp3) <- "KNN"
imp <- data.frame(varimp1, varimp2, varimp3)
head(imp[order(imp$CUBE, decreasing = T), ], 15)
```

#### Interpretation

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

### Exercise 8.7.c

Plot the optimal single tree with the distribution of yield in the terminal nodes. Does this view of the data provide additional knowledge about the biological or process predictors and their relationship with yield?

#### Approach

This `plot()` method for single tree objects is an extensible framework for the visualization of binary regression trees.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
plot(tune4$finalModel)
```

#### Interpretation

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

# References

https://rpubs.com/josezuniga/376348

http://appliedpredictivemodeling.com/

https://github.com/topepo/APM_Exercises

https://www.ncbi.nlm.nih.gov/pubmed/18620558

https://cran.r-project.org/web/packages/gbm/gbm.pdf

https://cran.r-project.org/web/packages/caret/caret.pdf

https://cran.r-project.org/web/packages/RWeka/RWeka.pdf

https://topepo.github.io/caret/train-models-by-tag.html#Bagging

https://github.com/topepo/caret/blob/master/RegressionTests/Code/M5.R

https://github.com/topepo/caret/blob/master/RegressionTests/Code/bag.R

http://appliedpredictivemodeling.com/blog/2014/11/12/solutions-on-github 

https://github.com/topepo/caret/blob/master/RegressionTests/Code/ctree2.R

http://www.rdocumentation.org/packages/caret/versions/6.0-79/topics/train

https://github.com/topepo/caret/blob/master/RegressionTests/Code/cforest.R

https://www.rdocumentation.org/packages/party/versions/1.2-4/topics/varimp

https://www.rdocumentation.org/packages/caret/versions/6.0-78/topics/varImp

https://cran.r-project.org/web/packages/HSAUR2/vignettes/Ch_recursive_partitioning.pdf
