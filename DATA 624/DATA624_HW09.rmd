---
title: "Rules-Based Models"
author: "Jose Zuniga"
output:
  html_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
if (!require("mlbench")) install.packages("mlbench")
library(mlbench)
```

## Exercise 8.4

Use a single predictor in the solubility data, such as the molecular weight or the number of carbon atoms and fit several models:

### Exercise 8.4.a

A simple regression tree.

#### Approach

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

#### Results

```{r fig.align='center', cache=TRUE}
```

#### Interpretation

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

### Exercise 8.4.b

A random forest model.

#### Approach

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

#### Results

```{r fig.align='center', cache=TRUE}
```

#### Interpretation

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

### Exercise 8.4.c

Different Cubist models with a single rule or multiple committees (each with and without using neighbor adjustments).

#### Approach

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

#### Results

```{r fig.align='center', cache=TRUE}
```

#### Interpretation

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

### Exercise 8.4.d

Plot the predictor data versus the solubility results for the test set. Overlay the model predictions for the test set. How do the model differ? Does changing the tuning parameter(s) significantly affect the model fit?

#### Approach

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

#### Results

```{r fig.align='center', cache=TRUE}
```

#### Interpretation

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

## Exercise 8.5

Fit different tree- and rule-based models for the Tecator data discussed in Exercise 6.1. How do they compare to linear models? Do the betweenpredictor correlations seem to affect your models? If so, how would you transform or re-encode the predictor data to mitigate this issue?

#### Approach

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

#### Results

```{r fig.align='center', cache=TRUE}
```

#### Interpretation

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

## Exercise 8.6

Return to the permeability problem described in Exercises 6.2 and 7.4. Train several tree-based models and evaluate the resampling and test set performance:

### Exercise 8.6.a

Which tree-based model gives the optimal resampling and test set performance?
#### Approach

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

#### Results

```{r fig.align='center', cache=TRUE}
```

#### Interpretation

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

### Exercise 8.6.b

Do any of these models outperform the covariance or non-covariance based regression models you have previously developed for these data? What criteria did you use to compare models' performance?

#### Approach

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

#### Results

```{r fig.align='center', cache=TRUE}
```

#### Interpretation

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

### Exercise 8.6.c

Of all the models you have developed thus far, which, if any, would you recommend to replace the permeability laboratory experiment?c

#### Approach

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

#### Results

```{r fig.align='center', cache=TRUE}
```

#### Interpretation

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

# References

http://appliedpredictivemodeling.com/

http://appliedpredictivemodeling.com/blog/2014/11/12/solutions-on-github 