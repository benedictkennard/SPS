---
title: "Nonlinear Regression Models"
author: "Jose Zuniga"
output:
  html_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r warning=F, message=F, fig.align='center', cache=T}
if (!require("pls")) install.packages("pls")
if (!require("nnet")) install.packages("nnet")
if (!require("caret")) install.packages("caret")
if (!require("earth")) install.packages("earth")
if (!require("mlbench")) install.packages("mlbench")
if (!require("AppliedPredictiveModeling")) install.packages("AppliedPredictiveModeling")
library(pls)
library(nnet)
library(earth)
library(caret)
library(mlbench)
library(AppliedPredictiveModeling)
```

**Note**: Model descriptions are from *Applied Predictive Modeling* by Max Kuhn and Kjell Johnson. Solutions are modifications of those posted by Max Kuhn on his public [GitHub](https://github.com/topepo/APM_Exercises) page. Function descriptions are from the [RDocumentation](https://www.rdocumentation.org) website. 

## Artificial Neural Networks (ANN)

Neural networks are powerful nonlinear regression techniques inspired by theories about how the brain works. Like [Partial Least squares](http://rpubs.com/josezuniga/366918) (PLS), the outcome is modeled by an intermediary set of unobserved variables (called hidden variables, hidden units, or hidden neurons). These hidden neurons are linear combinations of the original predictors. Unlike PLS models however, there are no constraints that help define these linear combinations and the linear combinations are not estimated in a hierarchical fashion. See the exercise on [this page](http://rpubs.com/josezuniga/340596) for further details on this technique.

## Multivariate Adaptive Regression Splines (MARS)

Like [Neural Networks](http://rpubs.com/josezuniga/340596) and [Partial Least squares](http://rpubs.com/josezuniga/366918), MARS uses surrogate features instead of the original predictors. However, whereas PLS and Neural Networks are based on linear combinations of the predictors, MARS creates two contrasted versions of a predictor to enter the model. Also, the surrogate features in MARS are usually a function of only one or two predictors at a time. The nature of the MARS features breaks the predictor into two groups and models linear relationships between the predictor and the outcome in each group. In effect, this scheme creates a piecewise linear model where each new feature models an isolated portion of the original data. Each data point for each predictor is evaluated as a candidate cut point (hinge) by creating a linear regression model with the candidate features, and the corresponding model error is calculated. The predictor-hinge combination that achieves the smallest error is then used for the model.

## Support Vector Machines (SVM)

SVMs are a class of powerful, highly flexible modeling techniques. The theory behind SVMs was originally developed in the [context of classification models](https://rpubs.com/josezuniga/223076).

> Support Vector Machine (SVM) is currently one of the most well-known and most commonly applied classifiers in supervised learning. The SVM employs a spatial representation of the data. SVMs fit vectors between the document features that best separate the documents into the various groups. Specifically, vectors are selected in a way that they maximize the space between the groups. After the estimation new documents are classified by checking on which sides of the vectors the features of unlabeled documents come to lie.

There are several flavors of Support Vector Regression. The focus here is on $\epsilon$-insensitive regression which is a robust regression technique which seeks to minimize the effect of outliers on the regression equations. Recall that linear regression seeks to find parameter estimates that minimize SSE. One drawback of minimizing SSE is that the parameter estimates can be influenced by just one observation that falls far from the overall trend in the data. When data may contain influential observations, an alternative minimization metric that is less sensitive can be used to find the best parameter estimates. The Huber function uses the squared residuals when the residuals are "small" and uses the absolute residuals when the residuals are large. SVMs for regression use a function similar to the Huber function, with an important difference. Data points with residuals within a threshold set by the user (denoted as $\epsilon$) do not contribute to the regression fit. Data points with an absolute difference greater than the threshold contribute a linear-scale amount.

## $K$-Nearest Neighbors (KNN)

The KNN approach predicts a new sample using the $K$-closest samples from the training set. Unlike the other methods, KNN cannot be cleanly summarized by a model. Instead, its construction is solely based on the individual samples from the training data. To predict a new sample for regression, KNN identifies that sample's KNNs in the predictor space. The predicted response for the new sample is then the mean of the $K$ neighbors' responses. Other summary statistics, such as the median, can also be used in place of the mean to predict the new sample.

## Exercise 7.2

Friedman (1991) introduced several benchmark data sets created by simulation. One of these simulations used the following nonlinear equation to create data: $y=10\sin { \left( \pi x_{ 1 }x_{ 2 } \right)  } +20\left( x_{ 3 }-0.5 \right) ^{ 2 }+10x_{ 4 }+5x_{ 5 }+N(0,\sigma ^{ 2 })$ where the $x$ values are random variables uniformly distributed between $[0,1]$ (there are also 5 other non-informative variables also created in the simulation). The package `mlbench` contains a function called `mlbench.friedman1` that simulates these data. [The function] creates a list with a vector $y$ and a matrix of predictors $x$. The $x$ data [are converted] from a matrix to a data frame [to, amongst other reasons,] give the columns names.

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
trainingData$x <- data.frame(trainingData$x)
featurePlot(trainingData$x, trainingData$y, layout=c(5,2))
```

A large test set [is simulated] to estimate the true error rate with good precision: [Then,] several models [are tuned] on these data.

```{r warning=F, message=F, fig.align='center', cache=T}
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
set.seed(921)
(knnModel <- train(x = trainingData$x, y = trainingData$y,
  method = "knn", preProc = c("center", "scale"), tuneLength = 10))
```

The function `postResample` [is] used to get the test set perforamnce [sic] values.

```{r warning=F, message=F, fig.align='center', cache=T}
knnPred <- predict(knnModel, newdata = testData$x)
postResample(pred = knnPred, obs = testData$y)
```

Which models appear to give the best performance? Does MARS select the informative predictors (those named `X1`-`X5`)?

#### Approach

In addition to the KNN models tuned on these data in the question setup, multiple MARS models are tuned the answer the question specific to MARS models. The `expand.grid()` function create a data frame consisting of all combinations (Cartesian product) of the supplied vectors or factors. The `train()` function sets up a grid of tuning parameters for a number of classification and regression routines, fits each model, and calculates a resampling based performance measure.  The `tuneGrid` parameter of the `train()` function is fed a data frame with values in columns named after the parameters being tuned. The `predict()` function applies a fitted model to data consisting of predictor variables. The `postResample()` function takes two vectors as its inputs and computes the RMSE, $R^2$, and MAE performance metrics. The `varImp()` function calculates the variable importance for a given model.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tg <- expand.grid(degree = 1:2, nprune = seq(2,14,by=2))
(tune <- train(x = trainingData$x, y = trainingData$y, method = "earth",
  preProcess = c("center", "scale"), tuneGrid = tg))
plot(tune)
fcast <- predict(tune, newdata = testData$x)
postResample(pred = fcast, obs = testData$y)
varImp(tune)
```

#### Interpretation

The MARS models outperform the KNN models tuned in the question setup. This is true by every performance metric as most of the MARS models have a lower RMSE, higher $R^2$, and lower MAE. MARS does select the informative predictors `X1`-`X5` and assigns zero importance to the non-informative predictors `X6`-`X10`. A second-degree MARS model with 14 terms (including the intercept) provides the most appropriate fit for these data. The second degree implies that a product of two variables is included as one or more of the terms in the model.

## Exercise 7.5

Exercise [6.3](http://rpubs.com/josezuniga/366918) describes data for a chemical manufacturing process. Use the same data imputation, data splitting, and pre-processing steps as before and train several nonlinear regression models.

> This data set contains information about a chemical manufacturing process... Raw material in this process is put through a sequence of 27 steps to generate the final pharmaceutical product. The starting material is generated from a biological unit and has a range of quality and characteristics... The data set consisted of 177 samples of biological material for which 57 characteristics were measured. Of the 57 characteristics, there were 12 measurements of the biological starting material, and 45 measurements of the manufacturing process. The process variables included measurements such as temperature, drying time, washing time, and concentrations of by-products at various steps. Some of the process measurements can be controlled, while others are observed. Samples are not independent because sets of samples come from the same batch of biological starting material.

```{r warning=F, message=F, fig.align='center', cache=T}
CMP <- get(data(ChemicalManufacturingProcess))
rm(ChemicalManufacturingProcess) # list not needed
set.seed(624)
rows_train <- createDataPartition(CMP$Yield, p=0.75, list=F)
CMP_train <- CMP[rows_train, ]
CMP_test <- CMP[-rows_train, ]
prepro <- preProcess(subset(CMP_train, select = - Yield),
  method=c("BoxCox", "center", "scale", "knnImpute"))
CMP_train_X <- predict(prepro, CMP_train[ , -1])
nzv <- nearZeroVar(CMP_train_X)
mcl <- findCorrelation(cor(CMP_train_X))
CMP_train_X <- CMP_train_X[ ,-c(nzv, mcl)] 
set.seed(624)
ctrl <- trainControl(method = "boot", number = 25)
tune1 <- train(x = CMP_train_X, y = CMP_train$Yield,
  method="pls", tuneLength=15, trControl=ctrl)
```

#### Artificial Neural Networks (ANN)

The [`avNNet`](https://www.rdocumentation.org/packages/caret/versions/6.0-78/topics/avNNet) function uses model averaging unlike the [`Nnet`](https://www.rdocumentation.org/packages/RcmdrPlugin.BCA/versions/0.9-8/topics/Nnet) function which creates a single model. The `.bag = FALSE` option makes the model use different random seeds instead of bagging (**b**ootstrap **agg**regation). For regression, the linear relationship between the hidden neurons and the prediction can be used with the option `linout = TRUE`. To reduce the amount of printed output use `trace = FALSE`. The number of neurons used by the model is specified in the `MaxNWts` parameter. The parameter `maxit = 500` expands the number of iterations to find neuron estimates.  

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tg2 <- expand.grid(.decay = c(0, 0.01, .1), .size = c(1:10), .bag = F)
tune2 <- train(x = CMP_train_X, y = CMP_train$Yield,
  method = "avNNet", tuneGrid = tg2, trControl = ctrl, linout = T, 
  trace = F, MaxNWts = 10 * (ncol(CMP_train_X) + 1) + 10 + 1, maxit = 500)
```

#### Multivariate Adaptive Regression Splines (MARS)

MARS models are [available] in several packages, but the most extensive implementation is in the [`earth`](https://www.rdocumentation.org/packages/earth/versions/4.6.0/topics/earth) package. 

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tg3 <- expand.grid(degree = c(1:2), nprune = c(2:10))
tune3 <- train(x = CMP_train_X, y = CMP_train$Yield,
  method = "earth", tuneGrid = tg3, trControl = ctrl)
```

#### Support Vector Machines (SVM)

The `kernlab` package has comprehensive implementation of SVM models for regression. Its [`ksvm`](https://www.rdocumentation.org/packages/kernlab/versions/0.9-25/topics/ksvm) function is available for regression models and uses the Guassian radial basis (`rbfdot`) function as the default kernel function. Other kernel functions can be used, including the polynomial (`polydot`) and linear (`vanilladot`). If the appropriate values of the cost and kernel parameters are unknown, they can be estimated through resampling. In `train`, the `method` values of `svmRadial`, `svmLinear`, or `svmPoly` fit different kernels.

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(614)
tg4 <- expand.grid(C=c(0.01,0.05,0.1), degree=c(1,2), scale=c(0.25,0.5,1))
tune4 <- train(x = CMP_train_X, y = CMP_train$Yield,
  method = "svmPoly",  tuneGrid = tg4,  trControl = ctrl)
```

#### $K$-Nearest Neighbors (KNN)

The [`knnreg`](https://www.rdocumentation.org/packages/caret/versions/6.0-78/topics/knnreg) function in the caret package fits the KNN regression model; train tunes the model over $K$.

```{r warning=F, message=F, fig.align='center', cache=T}
set.seed(624)
tg5 <- data.frame(.k = 1:20)
tune5 <- train(x = CMP_train_X, y = CMP_train$Yield,
  method = "knn", tuneGrid = tg5, trControl = trainControl(method = "cv"))
```

### Exercise 7.5.a

Which nonlinear regression model gives the optimal resampling and test set performance?

#### Approach

The pre-processing steps applied to the training set are applied to the training set. Then the same non-zero variance and highly correlated columns removed from the training set are removed from the test set. A slight issue arises during forecasting due to a value of $-\infty$ remaining in the pre-processed dataset. This infinite value hinders forecasting with the Neural Network and $K$-Nearest Neighbor models but does not affect the Multivariate Adaptive Regression Splines and Support Vector Machine models. For the sake of completeness in model comparison, the value of $-\infty$ is replaced with the minimum finite value of the variable to which the infinite value belongs. The `predict()` function applies a fitted model to data consisting of predictor variables. The `postResample()` function takes two vectors as its inputs and computes the RMSE, $R^2$, and MAE performance metrics. 

#### Results

```{r warning=F, message=F, fig.align='center', cache=T, results = 'hold'}
tune1$modelInfo$label
data.frame(model="PLS", tune1$bestTune, RMSE=min(tune1$results$RMSE), row.names="")
plot(tune1)
```
```{r warning=F, message=F, fig.align='center', cache=T, results = 'hold'}
tune2$modelInfo$label
data.frame(model="ANN", tune2$bestTune, RMSE=min(tune2$results$RMSE), row.names="")
plot(tune2)
```
```{r warning=F, message=F, fig.align='center', cache=T, results = 'hold'}
tune3$modelInfo$label
data.frame(model="MARS", tune3$bestTune, RMSE=min(tune3$results$RMSE), row.names="")
plot(tune3)
```
```{r warning=F, message=F, fig.align='center', cache=T, results = 'hold'}
tune4$modelInfo$label
data.frame(model="SVM", tune4$bestTune, RMSE=min(tune4$results$RMSE), row.names="")
plot(tune4)
```
```{r warning=F, message=F, fig.align='center', cache=T, results = 'hold'}
tune5$modelInfo$label
data.frame(model="KNN", tune5$bestTune, RMSE=min(tune5$results$RMSE), row.names="")
plot(tune5)
```

##### Training Set Resampling

```{r warning=F, message=F, fig.align='center', cache=T, results = 'hold'}
metrics <- function(tune) {
  RMSE = min(tune$results$RMSE)
  Rsquared = max(tune$results$Rsquared)
  MAE = min(tune$results$MAE)
  return(cbind(RMSE, Rsquared, MAE)) }
data.frame(rbind(metrics(tune1), metrics(tune2), 
  metrics(tune3), metrics(tune4), metrics(tune5)),
  row.names = c("PLS","ANN","MARS","SVM","KNN"))
```

##### Validation on Test Set 

```{r warning=F, message=F, fig.align='center', cache=T}
CMP_test_X <- predict(prepro, CMP_test[ , -1])
CMP_test_X <- CMP_test_X[ ,-c(nzv, mcl)] 
CMP_test_X[CMP_test_X[,35] == -Inf, 35] <- min(CMP_test_X[is.finite(CMP_test_X[,35]), 35])
fcast1 <- predict(tune1, newdata = CMP_test_X)
fcast2 <- predict(tune2, newdata = CMP_test_X)
fcast3 <- predict(tune3, newdata = CMP_test_X)
fcast4 <- predict(tune4, newdata = CMP_test_X)
fcast5 <- predict(tune5, newdata = CMP_test_X)
data.frame(rbind(postResample(pred = fcast1, obs = CMP_test$Yield), 
  postResample(pred = fcast2, obs = CMP_test$Yield), 
  postResample(pred = fcast3, obs = CMP_test$Yield),
  postResample(pred = fcast4, obs = CMP_test$Yield),
  postResample(pred = fcast5, obs = CMP_test$Yield)), 
  row.names = c("PLS","ANN","MARS","SVM","KNN"))
```

#### Interpretation

The optimal RMSE, $R^2$, and MAE resampling performance metrics are associated with the KNN model followed by the MARS, PLS, SVM, and ANN models in that order. The minimum RMSE has a range of 0.35 between the KNN resampled model and the ANN resampled model. The RMSE of the KNN, MARS, and PLS resampled models cluster around the low end of the range. The RMSE of the SVM resampled model sits around the midpoint of the range. At the high end of the range rests the RMSE of the ANN resampled model. The optimal RMSE, $R^2$, and MAE test set performance metrics are also associated with the KNN model. The range of the RMSE between the best and worst performing models is much larger for the test set because performance metrics from resampled training sets are highly optimistic as a result of the repeated sampling. These results are interesting because the author states:

> $K$-Nearest Neighbors models have better performance when the underlying relationship between predictors and the response relies is dependent on samples' proximity in the predictor space. Geographic information is not part of the data generation scheme for this particular data set. Hence, we would expect another type of model to perform better then KNN.

### Exercise 7.5.b

Which predictors are most important in the optimal nonlinear regression model? Do either the biological or process variables dominate the list? How do the top ten important predictors compare to the top ten predictors from the optimal linear model?

#### Approach

The `dotPlot()` function create a dotplot of variable importance values. The `varImp()` function calculates the variable importance for a given model.

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
dotPlot(varImp(tune5), top=15)
imp <- data.frame(varImp(tune5)$importance, varImp(tune1)$importance)
colnames(imp) <- c("KNN","PLS")
head(imp[order(imp$KNN,decreasing = T), ], 15)
```

#### Interpretation

The KNN model which was found to be the optimal nonlinear regression model and the PLS model which [was found](http://rpubs.com/josezuniga/366918) to be the optimal linear model agree that the most important predictors are process variables and that `ManufacturingProcess32` is the most important predictor. The ranking of the other predictors is very different however. When looking at the top 15 most important variables, the KNN model lists fewer biological process variables, but those fewer biological processes are of more importance in the KNN model than the PLS model.

### Exercise 7.5.c

Explore the relationships between the top predictors and the response for the predictors that are unique to the optimal nonlinear regression model. Do these plots reveal intuition about the biological or process predictors and their relationship with yield?

#### Approach

The `featurePlot()` function produces lattice graphs containing scatter plots of the specified predictor variables against the target variable. Correlation measures the relationship between two variables. The most commonly used measure of correlation is Pearson correlation ($r$). Pearson correlation measures the relationship between normal linear homoskedastic variables. Measuring relationships between variables that are not normal, linear, or homoskedastic (inherently or through transformation) with the Pearson correlation formula produces misleading results. Spearman correlation ($\rho$) and Kendall correlation ($\tau$) are non-parametric measures of correlation based on monotonic rank. Spearman correlation is more computationally efficient than Kendall correlation, but less robust. For this analysis, the accuracy of Kendall correlation is being prioritized over the speed of Spearman correlation. 

$$r = \frac{\sum{(x-m_x)(y-m_y)}}{\sqrt{\sum{(x-m_x)^2}\sum{(y-m_y)^2}}} \tag{Pearson}$$
$$\rho = \frac{\sum(x' - m_{x'})(y'_i - m_{y'})}{\sqrt{\sum(x' - m_{x'})^2 \sum(y' - m_{y'})^2}} \tag{Spearman}$$
$$\tau = \frac{n_c - n_d}{\frac{1}{2}n(n-1)} \tag{Kendall}$$

#### Results

```{r warning=F, message=F, fig.align='center', cache=T}
important <- c('ManufacturingProcess32',
               'ManufacturingProcess06',
               'ManufacturingProcess31',
               'ManufacturingProcess13')
featurePlot(CMP_train_X[, important], CMP_train$Yield)
cor(CMP_train_X[, important], CMP_train$Yield, method="kendall")
```

#### Interpretation

The top four predictors show low to moderate correlation with the response variable. Some are negatively correlated and others are positively correlated. The intuition that is potentially revealed about the predictors and their relationship with yield is how each predictor plays a small part in the yield. This would explain the need for a 27-step process. It also highlights the importance of selecting the appropriate biological material for the pharmaceutical product.

# References

https://www.rdocumentation.org/

http://rpubs.com/josezuniga/366918

http://rpubs.com/josezuniga/340596

https://rpubs.com/josezuniga/223076

http://appliedpredictivemodeling.com/

https://github.com/topepo/APM_Exercises

http://appliedpredictivemodeling.com/blog/2014/11/12/solutions-on-github

https://www.rdocumentation.org/packages/earth/versions/4.6.0/topics/earth

https://www.rdocumentation.org/packages/caret/versions/6.0-78/topics/avNNet

https://www.rdocumentation.org/packages/kernlab/versions/0.9-25/topics/ksvm

https://www.rdocumentation.org/packages/caret/versions/6.0-78/topics/knnreg

https://www.rdocumentation.org/packages/RcmdrPlugin.BCA/versions/0.9-8/topics/Nnet

https://www.rdocumentation.org/packages/AppliedPredictiveModeling/versions/1.1-6/topics/ChemicalManufacturingProcess
